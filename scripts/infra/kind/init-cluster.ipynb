{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kubernetes in /storage/donatien/.local/lib/python3.8/site-packages (23.3.0)\n",
      "Requirement already satisfied: tqdm in /storage/donatien/.local/lib/python3.8/site-packages (4.64.0)\n",
      "Requirement already satisfied: pandas in /storage/donatien/.local/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: nbformat in /storage/donatien/.local/lib/python3.8/site-packages (5.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /storage/donatien/.local/lib/python3.8/site-packages (from kubernetes) (6.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /storage/donatien/.local/lib/python3.8/site-packages (from kubernetes) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /storage/donatien/.local/lib/python3.8/site-packages (from kubernetes) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /storage/donatien/.local/lib/python3.8/site-packages (from kubernetes) (2.6.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from kubernetes) (1.14.0)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/lib/python3/dist-packages (from kubernetes) (1.25.8)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/lib/python3/dist-packages (from kubernetes) (2019.11.28)\n",
      "Requirement already satisfied: requests-oauthlib in /storage/donatien/.local/lib/python3.8/site-packages (from kubernetes) (1.3.1)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/lib/python3/dist-packages (from kubernetes) (45.2.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from kubernetes) (2.22.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /storage/donatien/.local/lib/python3.8/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /storage/donatien/.local/lib/python3.8/site-packages (from pandas) (1.22.3)\n",
      "Requirement already satisfied: traitlets>=4.1 in /storage/donatien/.local/lib/python3.8/site-packages (from nbformat) (5.3.0)\n",
      "Requirement already satisfied: jupyter-core in /storage/donatien/.local/lib/python3.8/site-packages (from nbformat) (4.10.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/lib/python3/dist-packages (from nbformat) (3.2.0)\n",
      "Requirement already satisfied: fastjsonschema in /storage/donatien/.local/lib/python3.8/site-packages (from nbformat) (2.15.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /storage/donatien/.local/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /storage/donatien/.local/lib/python3.8/site-packages (from google-auth>=1.0.1->kubernetes) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth>=1.0.1->kubernetes) (0.2.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib->kubernetes) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth>=1.0.1->kubernetes) (0.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%run ../common/common.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution: Nodes should have been already in `cluster.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind delete cluster\n",
      "kind delete cluster\n",
      "Log 0 - 2022-07-18 18:51:08 : Deleting cluster \"kind\" ...\n",
      "kind create cluster --config=cluster.yaml\n",
      "kind create cluster --config=cluster.yaml\n",
      "Log 0 - 2022-07-18 18:51:11 : Creating cluster \"kind\" ...\n",
      "Log 0 - 2022-07-18 18:51:11 : • Ensuring node image (kindest/node:v1.21.12) 🖼  ...\n",
      "Log 0 - 2022-07-18 18:51:11 : ✓ Ensuring node image (kindest/node:v1.21.12) 🖼\n",
      "Log 0 - 2022-07-18 18:51:11 : • Preparing nodes 📦 📦 📦 📦   ...\n",
      "Log 0 - 2022-07-18 18:51:17 : ✓ Preparing nodes 📦 📦 📦 📦\n",
      "Log 0 - 2022-07-18 18:51:17 : • Writing configuration 📜  ...\n",
      "Log 0 - 2022-07-18 18:51:18 : ✓ Writing configuration 📜\n",
      "Log 0 - 2022-07-18 18:51:18 : • Starting control-plane 🕹️  ...\n",
      "Log 0 - 2022-07-18 18:51:29 : ✓ Starting control-plane 🕹️\n",
      "Log 0 - 2022-07-18 18:51:29 : • Installing CNI 🔌  ...\n",
      "Log 0 - 2022-07-18 18:51:31 : ✓ Installing CNI 🔌\n",
      "Log 0 - 2022-07-18 18:51:31 : • Installing StorageClass 💾  ...\n",
      "Log 0 - 2022-07-18 18:51:31 : ✓ Installing StorageClass 💾\n",
      "Log 0 - 2022-07-18 18:51:31 : • Joining worker nodes 🚜  ...\n",
      "Log 0 - 2022-07-18 18:52:04 : ✓ Joining worker nodes 🚜\n",
      "Log 0 - 2022-07-18 18:52:04 : Set kubectl context to \"kind-kind\"\n",
      "Log 0 - 2022-07-18 18:52:04 : You can now use your cluster with:\n",
      "Log 0 - 2022-07-18 18:52:04 : \n",
      "Log 0 - 2022-07-18 18:52:04 : kubectl cluster-info --context kind-kind\n",
      "Log 0 - 2022-07-18 18:52:04 : \n",
      "Log 0 - 2022-07-18 18:52:04 : Not sure what to do next? 😅  Check out https://kind.sigs.k8s.io/docs/user/quick-start/\n",
      "sleep 30\n",
      "sleep 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_command(\"kind delete cluster\", shell=False)\n",
    "run_command(\"kind create cluster --config=cluster.yaml\", shell=False)\n",
    "run_command(\"sleep 30\", shell=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../common/common_modules.sh\n",
      "../common/common_modules.sh\n",
      "Log 0 - 2025-01-14 10:55:36 : clusterrolebinding.rbac.authorization.k8s.io/manager-full created\n",
      "Log 0 - 2025-01-14 10:55:38 : customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created\n",
      "Log 0 - 2025-01-14 10:55:38 : customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created\n",
      "Log 0 - 2025-01-14 10:55:38 : customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created\n",
      "Log 0 - 2025-01-14 10:55:38 : customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created\n",
      "Log 0 - 2025-01-14 10:55:38 : customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created\n",
      "Log 0 - 2025-01-14 10:55:39 : customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created\n",
      "Log 0 - 2025-01-14 10:55:39 : namespace/cert-manager created\n",
      "Log 0 - 2025-01-14 10:55:39 : serviceaccount/cert-manager-cainjector created\n",
      "Log 0 - 2025-01-14 10:55:39 : serviceaccount/cert-manager created\n",
      "Log 0 - 2025-01-14 10:55:39 : serviceaccount/cert-manager-webhook created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrole.rbac.authorization.k8s.io/cert-manager-view created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrole.rbac.authorization.k8s.io/cert-manager-edit created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\n",
      "Log 0 - 2025-01-14 10:55:39 : clusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\n",
      "Log 0 - 2025-01-14 10:55:39 : role.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\n",
      "Log 0 - 2025-01-14 10:55:39 : role.rbac.authorization.k8s.io/cert-manager:leaderelection created\n",
      "Log 0 - 2025-01-14 10:55:39 : role.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\n",
      "Log 0 - 2025-01-14 10:55:39 : rolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\n",
      "Log 0 - 2025-01-14 10:55:39 : rolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created\n",
      "Log 0 - 2025-01-14 10:55:39 : rolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\n",
      "Log 0 - 2025-01-14 10:55:39 : service/cert-manager created\n",
      "Log 0 - 2025-01-14 10:55:39 : service/cert-manager-webhook created\n",
      "Log 0 - 2025-01-14 10:55:39 : deployment.apps/cert-manager-cainjector created\n",
      "Log 0 - 2025-01-14 10:55:39 : deployment.apps/cert-manager created\n",
      "Log 0 - 2025-01-14 10:55:39 : deployment.apps/cert-manager-webhook created\n",
      "Log 0 - 2025-01-14 10:55:39 : mutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\n",
      "Log 0 - 2025-01-14 10:55:39 : validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\n",
      "Log 0 - 2025-01-14 10:55:39 : Waiting for deployment \"cert-manager-webhook\" rollout to finish: 0 of 1 updated replicas are available...\n",
      "Log 0 - 2025-01-14 10:55:54 : deployment \"cert-manager-webhook\" successfully rolled out\n",
      "Log 0 - 2025-01-14 10:55:57 : namespace/flink-operator-system created\n",
      "Log 0 - 2025-01-14 10:55:57 : customresourcedefinition.apiextensions.k8s.io/flinkclusters.flinkoperator.k8s.io created\n",
      "Log 0 - 2025-01-14 10:55:57 : serviceaccount/flink-operator-controller-manager created\n",
      "Log 0 - 2025-01-14 10:55:57 : role.rbac.authorization.k8s.io/flink-operator-leader-election-role created\n",
      "Log 0 - 2025-01-14 10:55:57 : clusterrole.rbac.authorization.k8s.io/flink-operator-manager-role created\n",
      "Log 0 - 2025-01-14 10:55:57 : clusterrole.rbac.authorization.k8s.io/flink-operator-metrics-reader created\n",
      "Log 0 - 2025-01-14 10:55:57 : clusterrole.rbac.authorization.k8s.io/flink-operator-proxy-role created\n",
      "Log 0 - 2025-01-14 10:55:57 : rolebinding.rbac.authorization.k8s.io/flink-operator-leader-election-rolebinding created\n",
      "Log 0 - 2025-01-14 10:55:57 : clusterrolebinding.rbac.authorization.k8s.io/flink-operator-manager-rolebinding created\n",
      "Log 0 - 2025-01-14 10:55:57 : clusterrolebinding.rbac.authorization.k8s.io/flink-operator-proxy-rolebinding created\n",
      "Log 0 - 2025-01-14 10:55:57 : service/flink-operator-controller-manager-metrics-service created\n",
      "Log 0 - 2025-01-14 10:55:57 : service/flink-operator-webhook-service created\n",
      "Log 0 - 2025-01-14 10:55:57 : deployment.apps/flink-operator-controller-manager created\n",
      "Log 0 - 2025-01-14 10:55:57 : certificate.cert-manager.io/flink-operator-serving-cert created\n",
      "Log 0 - 2025-01-14 10:55:57 : issuer.cert-manager.io/flink-operator-selfsigned-issuer created\n",
      "Log 0 - 2025-01-14 10:55:57 : mutatingwebhookconfiguration.admissionregistration.k8s.io/flink-operator-mutating-webhook-configuration created\n",
      "Log 0 - 2025-01-14 10:55:57 : validatingwebhookconfiguration.admissionregistration.k8s.io/flink-operator-validating-webhook-configuration created\n",
      "Log 0 - 2025-01-14 10:55:57 : namespace/manager created\n",
      "Log 0 - 2025-01-14 10:55:58 : namespace/local-path-storage unchanged\n",
      "Log 0 - 2025-01-14 10:55:58 : serviceaccount/local-path-provisioner-service-account unchanged\n",
      "Log 0 - 2025-01-14 10:55:58 : clusterrole.rbac.authorization.k8s.io/local-path-provisioner-role unchanged\n",
      "Log 0 - 2025-01-14 10:55:58 : clusterrolebinding.rbac.authorization.k8s.io/local-path-provisioner-bind unchanged\n",
      "Log 0 - 2025-01-14 10:55:58 : deployment.apps/local-path-provisioner configured\n",
      "Log 0 - 2025-01-14 10:55:58 : storageclass.storage.k8s.io/local-path created\n",
      "Log 0 - 2025-01-14 10:55:58 : configmap/local-path-config configured\n",
      "Log 0 - 2025-01-14 10:55:58 : error: the path \"./cm-local-path.yaml\" does not exist\n",
      "Log 0 - 2025-01-14 10:55:58 : persistentvolumeclaim/pvc-manager-minio created\n",
      "Log 0 - 2025-01-14 10:56:31 : \"minio\" already exists with the same configuration, skipping\n",
      "Log 0 - 2025-01-14 10:56:32 : Hang tight while we grab the latest from your chart repositories...\n",
      "Log 0 - 2025-01-14 10:56:32 : ...Successfully got an update from the \"cloudhut\" chart repository\n",
      "Log 0 - 2025-01-14 10:56:32 : ...Successfully got an update from the \"flink-operator-repo\" chart repository\n",
      "Log 0 - 2025-01-14 10:56:32 : ...Successfully got an update from the \"minio\" chart repository\n",
      "Log 0 - 2025-01-14 10:56:32 : ...Successfully got an update from the \"grafana\" chart repository\n",
      "Log 0 - 2025-01-14 10:56:33 : ...Successfully got an update from the \"prometheus-community\" chart repository\n",
      "Log 0 - 2025-01-14 10:56:33 : Update Complete. ⎈Happy Helming!⎈\n",
      "Log 0 - 2025-01-14 10:56:55 : NAME: minio\n",
      "Log 0 - 2025-01-14 10:56:55 : LAST DEPLOYED: Tue Jan 14 10:56:33 2025\n",
      "Log 0 - 2025-01-14 10:56:55 : NAMESPACE: manager\n",
      "Log 0 - 2025-01-14 10:56:55 : STATUS: deployed\n",
      "Log 0 - 2025-01-14 10:56:55 : REVISION: 1\n",
      "Log 0 - 2025-01-14 10:56:55 : TEST SUITE: None\n",
      "Log 0 - 2025-01-14 10:56:55 : NOTES:\n",
      "Log 0 - 2025-01-14 10:56:55 : MinIO can be accessed via port 9000 on the following DNS name from within your cluster:\n",
      "Log 0 - 2025-01-14 10:56:55 : minio.manager.cluster.local\n",
      "Log 0 - 2025-01-14 10:56:55 : \n",
      "Log 0 - 2025-01-14 10:56:55 : To access MinIO from localhost, run the below commands:\n",
      "Log 0 - 2025-01-14 10:56:55 : \n",
      "Log 0 - 2025-01-14 10:56:55 : 1. export POD_NAME=$(kubectl get pods --namespace manager -l \"release=minio\" -o jsonpath=\"{.items[0].metadata.name}\")\n",
      "Log 0 - 2025-01-14 10:56:55 : \n",
      "Log 0 - 2025-01-14 10:56:55 : 2. kubectl port-forward $POD_NAME 9000 --namespace manager\n",
      "Log 0 - 2025-01-14 10:56:55 : \n",
      "Log 0 - 2025-01-14 10:56:55 : Read more about port forwarding here: http://kubernetes.io/docs/user-guide/kubectl/kubectl_port-forward/\n",
      "Log 0 - 2025-01-14 10:56:55 : \n",
      "Log 0 - 2025-01-14 10:56:55 : You can now access MinIO server on http://localhost:9000. Follow the below steps to connect to MinIO server with mc client:\n",
      "Log 0 - 2025-01-14 10:56:55 : \n",
      "Log 0 - 2025-01-14 10:56:55 : 1. Download the MinIO mc client - https://min.io/docs/minio/linux/reference/minio-mc.html#quickstart\n",
      "Log 0 - 2025-01-14 10:56:55 : \n",
      "Log 0 - 2025-01-14 10:56:55 : 2. export MC_HOST_minio-local=http://$(kubectl get secret --namespace manager minio -o jsonpath=\"{.data.rootUser}\" | base64 --decode):$(kubectl get secret --namespace manager minio -o jsonpath=\"{.data.rootPassword}\" | base64 --decode)@localhost:9000\n",
      "Log 0 - 2025-01-14 10:56:55 : \n",
      "Log 0 - 2025-01-14 10:56:55 : 3. mc ls minio-local\n",
      "Log 0 - 2025-01-14 10:56:55 : \"prometheus-community\" already exists with the same configuration, skipping\n",
      "Log 0 - 2025-01-14 10:56:55 : Hang tight while we grab the latest from your chart repositories...\n",
      "Log 0 - 2025-01-14 10:56:55 : ...Successfully got an update from the \"cloudhut\" chart repository\n",
      "Log 0 - 2025-01-14 10:56:55 : ...Successfully got an update from the \"minio\" chart repository\n",
      "Log 0 - 2025-01-14 10:56:55 : ...Successfully got an update from the \"flink-operator-repo\" chart repository\n",
      "Log 0 - 2025-01-14 10:56:56 : ...Successfully got an update from the \"grafana\" chart repository\n",
      "Log 0 - 2025-01-14 10:56:56 : ...Successfully got an update from the \"prometheus-community\" chart repository\n",
      "Log 0 - 2025-01-14 10:56:56 : Update Complete. ⎈Happy Helming!⎈\n",
      "Log 0 - 2025-01-14 10:57:26 : NAME: prom\n",
      "Log 0 - 2025-01-14 10:57:26 : LAST DEPLOYED: Tue Jan 14 10:57:01 2025\n",
      "Log 0 - 2025-01-14 10:57:26 : NAMESPACE: manager\n",
      "Log 0 - 2025-01-14 10:57:26 : STATUS: deployed\n",
      "Log 0 - 2025-01-14 10:57:26 : REVISION: 1\n",
      "Log 0 - 2025-01-14 10:57:26 : NOTES:\n",
      "Log 0 - 2025-01-14 10:57:26 : kube-prometheus-stack has been installed. Check its status by running:\n",
      "Log 0 - 2025-01-14 10:57:26 : kubectl --namespace manager get pods -l \"release=prom\"\n",
      "Log 0 - 2025-01-14 10:57:26 : \n",
      "Log 0 - 2025-01-14 10:57:26 : Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.\n",
      "Log 0 - 2025-01-14 10:57:29 : podmonitor.monitoring.coreos.com/flink-pod-monitor created\n",
      "Log 0 - 2025-01-14 10:57:29 : \"grafana\" already exists with the same configuration, skipping\n",
      "Log 0 - 2025-01-14 10:57:29 : Hang tight while we grab the latest from your chart repositories...\n",
      "Log 0 - 2025-01-14 10:57:29 : ...Successfully got an update from the \"cloudhut\" chart repository\n",
      "Log 0 - 2025-01-14 10:57:29 : ...Successfully got an update from the \"minio\" chart repository\n",
      "Log 0 - 2025-01-14 10:57:29 : ...Successfully got an update from the \"flink-operator-repo\" chart repository\n",
      "Log 0 - 2025-01-14 10:57:30 : ...Successfully got an update from the \"grafana\" chart repository\n",
      "Log 0 - 2025-01-14 10:57:30 : ...Successfully got an update from the \"prometheus-community\" chart repository\n",
      "Log 0 - 2025-01-14 10:57:30 : Update Complete. ⎈Happy Helming!⎈\n",
      "Log 0 - 2025-01-14 10:57:30 : Release \"loki\" does not exist. Installing it now.\n",
      "Log 0 - 2025-01-14 10:57:32 : W0114 10:57:32.565613   39998 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n",
      "Log 0 - 2025-01-14 10:57:32 : W0114 10:57:32.568235   39998 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n",
      "Log 0 - 2025-01-14 10:57:32 : W0114 10:57:32.620142   39998 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n",
      "Log 0 - 2025-01-14 10:57:32 : W0114 10:57:32.620189   39998 warnings.go:70] policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+\n",
      "Log 0 - 2025-01-14 10:57:32 : NAME: loki\n",
      "Log 0 - 2025-01-14 10:57:32 : LAST DEPLOYED: Tue Jan 14 10:57:32 2025\n",
      "Log 0 - 2025-01-14 10:57:32 : NAMESPACE: manager\n",
      "Log 0 - 2025-01-14 10:57:32 : STATUS: deployed\n",
      "Log 0 - 2025-01-14 10:57:32 : REVISION: 1\n",
      "Log 0 - 2025-01-14 10:57:32 : NOTES:\n",
      "Log 0 - 2025-01-14 10:57:32 : The Loki stack has been deployed to your cluster. Loki can now be added as a datasource in Grafana.\n",
      "Log 0 - 2025-01-14 10:57:32 : \n",
      "Log 0 - 2025-01-14 10:57:32 : See http://docs.grafana.org/features/datasources/loki/ for more detail.\n",
      "Log 0 - 2025-01-14 10:57:42 : namespace/kafka created\n",
      "Log 0 - 2025-01-14 10:57:43 : rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-entity-operator-delegation created\n",
      "Log 0 - 2025-01-14 10:57:43 : customresourcedefinition.apiextensions.k8s.io/strimzipodsets.core.strimzi.io created\n",
      "Log 0 - 2025-01-14 10:57:43 : clusterrole.rbac.authorization.k8s.io/strimzi-kafka-client created\n",
      "Log 0 - 2025-01-14 10:57:43 : customresourcedefinition.apiextensions.k8s.io/kafkausers.kafka.strimzi.io created\n",
      "Log 0 - 2025-01-14 10:57:43 : clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-broker-delegation created\n",
      "Log 0 - 2025-01-14 10:57:43 : configmap/strimzi-cluster-operator created\n",
      "Log 0 - 2025-01-14 10:57:43 : customresourcedefinition.apiextensions.k8s.io/kafkas.kafka.strimzi.io created\n",
      "Log 0 - 2025-01-14 10:57:43 : clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-namespaced created\n",
      "Log 0 - 2025-01-14 10:57:43 : customresourcedefinition.apiextensions.k8s.io/kafkatopics.kafka.strimzi.io created\n",
      "Log 0 - 2025-01-14 10:57:43 : customresourcedefinition.apiextensions.k8s.io/kafkaconnects.kafka.strimzi.io created\n",
      "Log 0 - 2025-01-14 10:57:43 : customresourcedefinition.apiextensions.k8s.io/kafkabridges.kafka.strimzi.io created\n",
      "Log 0 - 2025-01-14 10:57:43 : customresourcedefinition.apiextensions.k8s.io/kafkaconnectors.kafka.strimzi.io created\n",
      "Log 0 - 2025-01-14 10:57:43 : clusterrole.rbac.authorization.k8s.io/strimzi-entity-operator created\n",
      "Log 0 - 2025-01-14 10:57:43 : clusterrole.rbac.authorization.k8s.io/strimzi-cluster-operator-global created\n",
      "Log 0 - 2025-01-14 10:57:43 : clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator-kafka-client-delegation created\n",
      "Log 0 - 2025-01-14 10:57:43 : customresourcedefinition.apiextensions.k8s.io/kafkamirrormakers.kafka.strimzi.io created\n",
      "Log 0 - 2025-01-14 10:57:43 : clusterrole.rbac.authorization.k8s.io/strimzi-kafka-broker created\n",
      "Log 0 - 2025-01-14 10:57:43 : customresourcedefinition.apiextensions.k8s.io/kafkamirrormaker2s.kafka.strimzi.io created\n",
      "Log 0 - 2025-01-14 10:57:43 : customresourcedefinition.apiextensions.k8s.io/kafkarebalances.kafka.strimzi.io created\n",
      "Log 0 - 2025-01-14 10:57:43 : serviceaccount/strimzi-cluster-operator created\n",
      "Log 0 - 2025-01-14 10:57:43 : deployment.apps/strimzi-cluster-operator created\n",
      "Log 0 - 2025-01-14 10:57:43 : clusterrolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created\n",
      "Log 0 - 2025-01-14 10:57:43 : rolebinding.rbac.authorization.k8s.io/strimzi-cluster-operator created\n",
      "Log 0 - 2025-01-14 10:57:43 : \"cloudhut\" already exists with the same configuration, skipping\n",
      "Log 0 - 2025-01-14 10:57:43 : Hang tight while we grab the latest from your chart repositories...\n",
      "Log 0 - 2025-01-14 10:57:43 : ...Successfully got an update from the \"cloudhut\" chart repository\n",
      "Log 0 - 2025-01-14 10:57:43 : ...Successfully got an update from the \"minio\" chart repository\n",
      "Log 0 - 2025-01-14 10:57:44 : ...Successfully got an update from the \"flink-operator-repo\" chart repository\n",
      "Log 0 - 2025-01-14 10:57:44 : ...Successfully got an update from the \"grafana\" chart repository\n",
      "Log 0 - 2025-01-14 10:57:44 : ...Successfully got an update from the \"prometheus-community\" chart repository\n",
      "Log 0 - 2025-01-14 10:57:44 : Update Complete. ⎈Happy Helming!⎈\n",
      "Log 0 - 2025-01-14 10:57:45 : ingress.networking.k8s.io/minio created\n",
      "Log 0 - 2025-01-14 10:57:45 : ingress.networking.k8s.io/grafana created\n",
      "Log 0 - 2025-01-14 10:57:45 : ingress.networking.k8s.io/prometheus created\n",
      "Log 0 - 2025-01-14 10:57:45 : ingress.networking.k8s.io/zeppelin created\n",
      "namespace/ingress-nginx created\n",
      "serviceaccount/ingress-nginx created\n",
      "serviceaccount/ingress-nginx-admission created\n",
      "role.rbac.authorization.k8s.io/ingress-nginx created\n",
      "role.rbac.authorization.k8s.io/ingress-nginx-admission created\n",
      "clusterrole.rbac.authorization.k8s.io/ingress-nginx created\n",
      "clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created\n",
      "rolebinding.rbac.authorization.k8s.io/ingress-nginx created\n",
      "rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created\n",
      "configmap/ingress-nginx-controller created\n",
      "service/ingress-nginx-controller created\n",
      "service/ingress-nginx-controller-admission created\n",
      "deployment.apps/ingress-nginx-controller created\n",
      "job.batch/ingress-nginx-admission-create created\n",
      "job.batch/ingress-nginx-admission-patch created\n",
      "ingressclass.networking.k8s.io/nginx created\n",
      "validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created\n"
     ]
    }
   ],
   "source": [
    "run_command('../common/common_modules.sh')\n",
    "!kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind-control-plane\t{'beta.kubernetes.io/arch': 'amd64', 'beta.kubernetes.io/os': 'linux', 'kubernetes.io/arch': 'amd64', 'kubernetes.io/hostname': 'kind-control-plane', 'kubernetes.io/os': 'linux', 'node-role.kubernetes.io/control-plane': '', 'node-role.kubernetes.io/master': '', 'node.kubernetes.io/exclude-from-external-load-balancers': ''}\n",
      "kind-worker\t{'beta.kubernetes.io/arch': 'amd64', 'beta.kubernetes.io/os': 'linux', 'ingress-ready': 'true', 'kubernetes.io/arch': 'amd64', 'kubernetes.io/hostname': 'kind-worker', 'kubernetes.io/os': 'linux', 'tier': 'manager'}\n",
      "kind-worker2\t{'beta.kubernetes.io/arch': 'amd64', 'beta.kubernetes.io/os': 'linux', 'kubernetes.io/arch': 'amd64', 'kubernetes.io/hostname': 'kind-worker2', 'kubernetes.io/os': 'linux', 'tier': 'jobmanager'}\n",
      "kind-worker3\t{'beta.kubernetes.io/arch': 'amd64', 'beta.kubernetes.io/os': 'linux', 'kubernetes.io/arch': 'amd64', 'kubernetes.io/hostname': 'kind-worker3', 'kubernetes.io/os': 'linux', 'tier': 'taskmanager'}\n"
     ]
    }
   ],
   "source": [
    "(manager_node, jobmanager_node, taskmanager_nodes) = get_label_nodes(ip_address=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:             grafana\n",
      "Labels:           <none>\n",
      "Namespace:        manager\n",
      "Address:          localhost\n",
      "Default backend:  default-http-backend:80 (<error: endpoints \"default-http-backend\" not found>)\n",
      "Rules:\n",
      "  Host                        Path  Backends\n",
      "  ----                        ----  --------\n",
      "  grafana.127-0-0-1.sslip.io  \n",
      "                              /   prom-grafana:80 (10.244.1.5:3000)\n",
      "Annotations:                  kubernetes.io/ingress.class: nginx\n",
      "                              nginx.ingress.kubernetes.io/proxy-body-size: 0\n",
      "Events:\n",
      "  Type    Reason  Age                From                      Message\n",
      "  ----    ------  ----               ----                      -------\n",
      "  Normal  Sync    20s (x2 over 20s)  nginx-ingress-controller  Scheduled for sync\n",
      "\n",
      "\n",
      "Name:             minio\n",
      "Labels:           <none>\n",
      "Namespace:        manager\n",
      "Address:          localhost\n",
      "Default backend:  default-http-backend:80 (<error: endpoints \"default-http-backend\" not found>)\n",
      "Rules:\n",
      "  Host                      Path  Backends\n",
      "  ----                      ----  --------\n",
      "  minio.127-0-0-1.sslip.io  \n",
      "                            /   minio:9000 (10.244.3.3:9000)\n",
      "Annotations:                kubernetes.io/ingress.class: nginx\n",
      "                            nginx.ingress.kubernetes.io/proxy-body-size: 0\n",
      "Events:\n",
      "  Type    Reason  Age                From                      Message\n",
      "  ----    ------  ----               ----                      -------\n",
      "  Normal  Sync    20s (x2 over 20s)  nginx-ingress-controller  Scheduled for sync\n",
      "\n",
      "\n",
      "Name:             prometheus\n",
      "Labels:           <none>\n",
      "Namespace:        manager\n",
      "Address:          localhost\n",
      "Default backend:  default-http-backend:80 (<error: endpoints \"default-http-backend\" not found>)\n",
      "Rules:\n",
      "  Host                           Path  Backends\n",
      "  ----                           ----  --------\n",
      "  prometheus.127-0-0-1.sslip.io  \n",
      "                                 /   prom-kube-prometheus-stack-prometheus:9090 (10.244.1.6:9090)\n",
      "Annotations:                     kubernetes.io/ingress.class: nginx\n",
      "                                 nginx.ingress.kubernetes.io/proxy-body-size: 0\n",
      "Events:\n",
      "  Type    Reason  Age                From                      Message\n",
      "  ----    ------  ----               ----                      -------\n",
      "  Normal  Sync    20s (x2 over 20s)  nginx-ingress-controller  Scheduled for sync\n",
      "\n",
      "\n",
      "Name:             zeppelin\n",
      "Labels:           <none>\n",
      "Namespace:        manager\n",
      "Address:          localhost\n",
      "Default backend:  default-http-backend:80 (<error: endpoints \"default-http-backend\" not found>)\n",
      "Rules:\n",
      "  Host                         Path  Backends\n",
      "  ----                         ----  --------\n",
      "  zeppelin.127-0-0-1.sslip.io  \n",
      "                               /   zeppelin-server:8080 (<error: endpoints \"zeppelin-server\" not found>)\n",
      "Annotations:                   kubernetes.io/ingress.class: nginx\n",
      "                               nginx.ingress.kubernetes.io/proxy-body-size: 0\n",
      "Events:\n",
      "  Type    Reason  Age                From                      Message\n",
      "  ----    ------  ----               ----                      -------\n",
      "  Normal  Sync    20s (x2 over 20s)  nginx-ingress-controller  Scheduled for sync\n",
      "\n",
      "\n",
      "Name:             zeppelin\n",
      "Labels:           <none>\n",
      "Namespace:        manager\n",
      "Address:          localhost\n",
      "Default backend:  default-http-backend:80 (<error: endpoints \"default-http-backend\" not found>)\n",
      "Rules:\n",
      "  Host                         Path  Backends\n",
      "  ----                         ----  --------\n",
      "  zeppelin.127-0-0-1.sslip.io  \n",
      "                               /   zeppelin-server:8080 (<error: endpoints \"zeppelin-server\" not found>)\n",
      "Annotations:                   kubernetes.io/ingress.class: nginx\n",
      "                               nginx.ingress.kubernetes.io/proxy-body-size: 0\n",
      "Events:\n",
      "  Type    Reason  Age                From                      Message\n",
      "  ----    ------  ----               ----                      -------\n",
      "  Normal  Sync    20s (x2 over 20s)  nginx-ingress-controller  Scheduled for sync\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe ing -n manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manager node: 172.23.0.3:30080\n",
      "Access to Minio: http://minio.127-0-0-1.nip.io:30080\n",
      "Access to Grafana: http://grafana.127-0-0-1.nip.io:30080\n",
      "Access to Prometheus: http://prometheus.127-0-0-1:30080\n",
      "Job manager address: 172.23.0.5\n",
      "Task manager addresses: 172.23.0.2\n"
     ]
    }
   ],
   "source": [
    "address = \"127-0-0-1\"\n",
    "print(\"Manager node: {}:30080\".format(manager_node))\n",
    "print(\"Access to Minio: http://minio.{}.nip.io:30080\".format(address))\n",
    "print(\"Access to Grafana: http://grafana.{}.nip.io:30080\".format(address))\n",
    "print(\"Access to Prometheus: http://prometheus.{}:30080\".format(address))\n",
    "print(\"Job manager address: {}\".format(jobmanager_node))\n",
    "print(\"Task manager addresses: {}\".format(taskmanager_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access to Minio: http://172.23.0.3:30900\n",
      "Access to Grafana: http://172.23.0.3:30300\n",
      "Access to Prometheus: http://172.23.0.3:30090\n",
      "Job manager address: 172.23.0.5\n",
      "Task manager addresses: 172.23.0.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Access to Minio: http://{}:30900\".format(manager_node))\n",
    "print(\"Access to Grafana: http://{}:30300\".format(manager_node))\n",
    "print(\"Access to Prometheus: http://{}:30090\".format(manager_node))\n",
    "print(\"Job manager address: {}\".format(jobmanager_node))\n",
    "print(\"Task manager addresses: {}\".format(taskmanager_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import [Flink dashboard](https://grafana.com/grafana/dashboards/14911) in Grafana."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
